{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\alex4321\\documents\\alpaca_lora_4bit\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting peft@ git+https://github.com/huggingface/peft.git@70af02a2bca5a63921790036b2c9430edf4037e2 (from alpaca-lora-4bit==0.1.2)\n",
      "  Using cached peft-0.3.0.dev0-py3-none-any.whl\n",
      "Collecting transformers@ git+https://github.com/huggingface/transformers.git (from alpaca-lora-4bit==0.1.2)\n",
      "  Cloning https://github.com/huggingface/transformers.git to c:\\users\\alex4321\\appdata\\local\\temp\\pip-install-i4zz5xjz\\transformers_5ff4772c58d644099279b0cb9c4f2959\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 1e9da2b0a6ef964c2cf72dd715dbee991a3f49fa\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: torch in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages (from alpaca-lora-4bit==0.1.2) (2.0.1+cu117)\n",
      "Requirement already satisfied: accelerate in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\accelerate-0.20.3-py3.10.egg (from alpaca-lora-4bit==0.1.2) (0.20.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\datasets-2.13.1-py3.10.egg (from alpaca-lora-4bit==0.1.2) (2.13.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\sentencepiece-0.1.99-py3.10-win-amd64.egg (from alpaca-lora-4bit==0.1.2) (0.1.99)\n",
      "Requirement already satisfied: safetensors in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\safetensors-0.3.1-py3.10-win-amd64.egg (from alpaca-lora-4bit==0.1.2) (0.3.1)\n",
      "Requirement already satisfied: einops in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\einops-0.6.1-py3.10.egg (from alpaca-lora-4bit==0.1.2) (0.6.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\colorama-0.4.6-py3.10.egg (from alpaca-lora-4bit==0.1.2) (0.4.6)\n",
      "Requirement already satisfied: pyzmq in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\pyzmq-25.1.1b1-py3.10-win-amd64.egg (from alpaca-lora-4bit==0.1.2) (25.1.1b1)\n",
      "Requirement already satisfied: wandb in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\wandb-0.15.4-py3.10.egg (from alpaca-lora-4bit==0.1.2) (0.15.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\packaging-23.1-py3.10.egg (from alpaca-lora-4bit==0.1.2) (23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\numpy-1.25.0-py3.10-win-amd64.egg (from accelerate->alpaca-lora-4bit==0.1.2) (1.25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\psutil-5.9.5-py3.10-win-amd64.egg (from accelerate->alpaca-lora-4bit==0.1.2) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\pyyaml-6.0-py3.10-win-amd64.egg (from accelerate->alpaca-lora-4bit==0.1.2) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages (from torch->alpaca-lora-4bit==0.1.2) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages (from torch->alpaca-lora-4bit==0.1.2) (4.4.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages (from torch->alpaca-lora-4bit==0.1.2) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages (from torch->alpaca-lora-4bit==0.1.2) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages (from torch->alpaca-lora-4bit==0.1.2) (3.1.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\pyarrow-12.0.1-py3.10-win-amd64.egg (from datasets->alpaca-lora-4bit==0.1.2) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\dill-0.3.6-py3.10.egg (from datasets->alpaca-lora-4bit==0.1.2) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\pandas-2.0.2-py3.10-win-amd64.egg (from datasets->alpaca-lora-4bit==0.1.2) (2.0.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\requests-2.31.0-py3.10.egg (from datasets->alpaca-lora-4bit==0.1.2) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\tqdm-4.65.0-py3.10.egg (from datasets->alpaca-lora-4bit==0.1.2) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\xxhash-3.2.0-py3.10-win-amd64.egg (from datasets->alpaca-lora-4bit==0.1.2) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\multiprocess-0.70.14-py3.10.egg (from datasets->alpaca-lora-4bit==0.1.2) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\fsspec-2023.6.0-py3.10.egg (from datasets->alpaca-lora-4bit==0.1.2) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages (from datasets->alpaca-lora-4bit==0.1.2) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\huggingface_hub-0.15.1-py3.10.egg (from datasets->alpaca-lora-4bit==0.1.2) (0.15.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\regex-2023.6.3-py3.10-win-amd64.egg (from transformers@ git+https://github.com/huggingface/transformers.git->alpaca-lora-4bit==0.1.2) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\tokenizers-0.13.3-py3.10-win-amd64.egg (from transformers@ git+https://github.com/huggingface/transformers.git->alpaca-lora-4bit==0.1.2) (0.13.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\click-8.1.3-py3.10.egg (from wandb->alpaca-lora-4bit==0.1.2) (8.1.3)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\gitpython-3.1.31-py3.10.egg (from wandb->alpaca-lora-4bit==0.1.2) (3.1.31)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\sentry_sdk-1.26.0-py3.10.egg (from wandb->alpaca-lora-4bit==0.1.2) (1.26.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\docker_pycreds-0.4.0-py3.10.egg (from wandb->alpaca-lora-4bit==0.1.2) (0.4.0)\n",
      "Requirement already satisfied: pathtools in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\pathtools-0.1.2-py3.10.egg (from wandb->alpaca-lora-4bit==0.1.2) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\setproctitle-1.3.2-py3.10-win-amd64.egg (from wandb->alpaca-lora-4bit==0.1.2) (1.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages (from wandb->alpaca-lora-4bit==0.1.2) (67.8.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\appdirs-1.4.4-py3.10.egg (from wandb->alpaca-lora-4bit==0.1.2) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\protobuf-4.23.3-py3.10.egg (from wandb->alpaca-lora-4bit==0.1.2) (4.23.3)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\six-1.16.0-py3.10.egg (from docker-pycreds>=0.4.0->wandb->alpaca-lora-4bit==0.1.2) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages (from aiohttp->datasets->alpaca-lora-4bit==0.1.2) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages (from aiohttp->datasets->alpaca-lora-4bit==0.1.2) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages (from aiohttp->datasets->alpaca-lora-4bit==0.1.2) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages (from aiohttp->datasets->alpaca-lora-4bit==0.1.2) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages (from aiohttp->datasets->alpaca-lora-4bit==0.1.2) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages (from aiohttp->datasets->alpaca-lora-4bit==0.1.2) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages (from aiohttp->datasets->alpaca-lora-4bit==0.1.2) (1.3.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\gitdb-4.0.10-py3.10.egg (from GitPython!=3.1.29,>=1.0.0->wandb->alpaca-lora-4bit==0.1.2) (4.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages (from requests>=2.19.0->datasets->alpaca-lora-4bit==0.1.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\urllib3-2.0.3-py3.10.egg (from requests>=2.19.0->datasets->alpaca-lora-4bit==0.1.2) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\certifi-2023.5.7-py3.10.egg (from requests>=2.19.0->datasets->alpaca-lora-4bit==0.1.2) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages (from jinja2->torch->alpaca-lora-4bit==0.1.2) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\python_dateutil-2.8.2-py3.10.egg (from pandas->datasets->alpaca-lora-4bit==0.1.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\pytz-2023.3-py3.10.egg (from pandas->datasets->alpaca-lora-4bit==0.1.2) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\tzdata-2023.3-py3.10.egg (from pandas->datasets->alpaca-lora-4bit==0.1.2) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages (from sympy->torch->alpaca-lora-4bit==0.1.2) (1.2.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\smmap-5.0.0-py3.10.egg (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->alpaca-lora-4bit==0.1.2) (5.0.0)\n",
      "Building wheels for collected packages: alpaca-lora-4bit\n",
      "  Building wheel for alpaca-lora-4bit (setup.py): started\n",
      "  Building wheel for alpaca-lora-4bit (setup.py): finished with status 'done'\n",
      "  Created wheel for alpaca-lora-4bit: filename=alpaca_lora_4bit-0.1.2-cp310-cp310-win_amd64.whl size=221342 sha256=b1f2b4f5bc41470f1a3ce17b4e385a29406018e26525e39909e03e5a6f891d5f\n",
      "  Stored in directory: C:\\Users\\alex4321\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-wpyu_s9y\\wheels\\f1\\16\\c8\\7b0283419f5db9fd5a62e7ef5c699fefbc9b0c025da8396cdf\n",
      "Successfully built alpaca-lora-4bit\n",
      "Installing collected packages: alpaca-lora-4bit\n",
      "  Attempting uninstall: alpaca-lora-4bit\n",
      "    Found existing installation: alpaca-lora-4bit 0.1.2\n",
      "    Uninstalling alpaca-lora-4bit-0.1.2:\n",
      "      Successfully uninstalled alpaca-lora-4bit-0.1.2\n",
      "Successfully installed alpaca-lora-4bit-0.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git 'C:\\Users\\alex4321\\AppData\\Local\\Temp\\pip-install-i4zz5xjz\\transformers_5ff4772c58d644099279b0cb9c4f2959'\n"
     ]
    }
   ],
   "source": [
    "!pip3 install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton not found. Please run \"pip install triton\"."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\tqdm-4.65.0-py3.10.egg\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using CUDA implementation.\n"
     ]
    }
   ],
   "source": [
    "from alpaca_lora_4bit.autograd_4bit import load_llama_model_4bit_low_ram, Autograd4bitQuantLinear, switch_backend_to\n",
    "\n",
    "switch_backend_to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca_lora_4bit import autograd_4bit\n",
    "from alpaca_lora_4bit import matmul_utils_4bit as mm4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _patched_linear_forward(self, x):\n",
    "    assert not torch.isnan(x).any().item()\n",
    "    assert not torch.isinf(x).any().item()\n",
    "    if self.bits == 4:\n",
    "        if torch.is_grad_enabled():\n",
    "            out = autograd_4bit.AutogradMatmul4bit.apply(x, self.qweight, self.scales,\n",
    "                                        self.qzeros if not self.is_v1_model else self.zeros,\n",
    "                                        self.g_idx, self.bits, self.maxq)\n",
    "            assert not torch.isnan(out).any().item()\n",
    "            assert not torch.isinf(out).any().item()\n",
    "        else:\n",
    "            out = autograd_4bit.matmul4bit_with_backend(x, self.qweight, self.scales,\n",
    "                                        self.qzeros if not self.is_v1_model else self.zeros,\n",
    "                                        self.g_idx, self.bits, self.maxq, self.groupsize)\n",
    "            assert not torch.isnan(out).any().item()\n",
    "            assert not torch.isinf(out).any().item()\n",
    "    elif self.bits == 2:\n",
    "        raise NotImplementedError(\"Debugging 4-bit case\")\n",
    "        out = AutogradMatmul2bit.apply(x, self.qweight, self.scales, self.qzeros, self.g_idx, self.bits, self.maxq)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Debugging 4-bit case\")\n",
    "        raise ValueError('Unsupported bitwidth.')\n",
    "    if not self.disable_bias:\n",
    "        assert not torch.isnan(self.bias).any().item()\n",
    "        assert not torch.isinf(self.bias).any().item()\n",
    "        out += self.bias\n",
    "        assert not torch.isnan(out).any().item()\n",
    "        assert not torch.isinf(out).any().item()\n",
    "    return out\n",
    "\n",
    "\n",
    "autograd_4bit.Autograd4bitQuantLinear.forward = _patched_linear_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _patched_matmul4bit_with_backend(x, qweight, scales, qzeros, g_idx, bits, maxq, groupsize=None):\n",
    "    assert not torch.isinf(x).any().item()\n",
    "    assert not torch.isnan(x).any().item()\n",
    "    out = mm4b.matmul4bit(x, qweight, scales, qzeros, g_idx, groupsize)\n",
    "    assert not torch.isinf(out).any().item()\n",
    "    assert not torch.isnan(out).any().item()\n",
    "    return out\n",
    "\n",
    "\n",
    "autograd_4bit.matmul4bit_with_backend = _patched_matmul4bit_with_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_forward_check_nan_inf(name, module):\n",
    "    def _func(*args, **kwargs):\n",
    "        input_tensors = [item for item in args if isinstance(item, torch.Tensor)] + \\\n",
    "            [item for item in kwargs.values() if isinstance(item, torch.Tensor)]\n",
    "        for tensor in input_tensors:\n",
    "            if torch.isinf(tensor).any().item():\n",
    "                raise ValueError(f\"Got Inf in {name} input\")\n",
    "            if torch.isnan(tensor).any().item():\n",
    "                raise ValueError(f\"Got NaN in {name} input\")\n",
    "        result = module._nan_check_old_forward(*args, **kwargs)\n",
    "        tensors = []\n",
    "        if isinstance(result, torch.Tensor):\n",
    "            tensors = result\n",
    "        if isinstance(result, tuple) or isinstance(result, list):\n",
    "            tensors = [item for item in result if isinstance(item, torch.Tensor)]\n",
    "        if isinstance(result, dict):\n",
    "            tensors = [item for item in result.values() if isinstance(item, torch.Tensor)]\n",
    "        for tensor in tensors:\n",
    "            if torch.isinf(tensor).any().item():\n",
    "                raise ValueError(f\"Got Inf in {name} output\")\n",
    "            if torch.isnan(tensor).any().item():\n",
    "                raise ValueError(f\"Got NaN in {name} output\")\n",
    "        return result\n",
    "    \n",
    "    return _func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_forward_check_nan_inf(module, name):\n",
    "    module._nan_check_old_forward = module.forward\n",
    "    module.forward = build_forward_check_nan_inf(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model in 6.38 seconds.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_llama_model_4bit_low_ram(\"vicuna-13B-1.1-GPTQ-4bit-128g\",\n",
    "                                                 \"vicuna-13B-1.1-GPTQ-4bit-128g/vicuna-13B-1.1-GPTQ-4bit-128g.compat.no-act-order.pt\",\n",
    "                                                 groupsize=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_forward_check_nan_inf(model.model.embed_tokens, \"embed_tokens\")\n",
    "for i in range(40):\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].self_attn.q_proj, f\"Layer {i} self-attention q_proj\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].self_attn.k_proj, f\"Layer {i} self-attention k_proj\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].self_attn.v_proj, f\"Layer {i} self-attention v_proj\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].self_attn.o_proj, f\"Layer {i} self-attention o_proj\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].self_attn.rotary_emb, f\"Layer {i} self-attention rotary_emb\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].self_attn, f\"Layer {i} self-attention itself\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].mlp.gate_proj, f\"Layer {i} mlp gate_proj\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].mlp.down_proj, f\"Layer {i} mlp down_proj\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].mlp.up_proj, f\"Layer {i} mlp up_proj\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].mlp.act_fn, f\"Layer {i} mlp act_fn\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].mlp, f\"Layer {i} mlp itself\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].input_layernorm, f\"Layer {i} input_layernorm\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].post_attention_layernorm, f\"Layer {i} post_attention_layernorm\")\n",
    "wrap_forward_check_nan_inf(model.model.norm, \"norm\")\n",
    "wrap_forward_check_nan_inf(model.lm_head, \"lm_head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.half()\n",
    "for n, m in model.named_modules():\n",
    "    if isinstance(m, Autograd4bitQuantLinear):\n",
    "        if m.is_v1_model:\n",
    "            m.zeros = m.zeros.half()\n",
    "        m.scales = m.scales.half()\n",
    "        m.bias = m.bias.half()\n",
    "model.tie_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca_lora_4bit.amp_wrapper import AMPWrapper\n",
    "wrapper = AMPWrapper(model)\n",
    "wrapper.apply_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''I think the meaning of life is'''\n",
    "batch = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "batch = {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m----> 2\u001b[0m      generated \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(inputs\u001b[39m=\u001b[39;49mbatch[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m      3\u001b[0m                                 do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, use_cache\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      4\u001b[0m                                 repetition_penalty\u001b[39m=\u001b[39;49m\u001b[39m1.1\u001b[39;49m,\n\u001b[0;32m      5\u001b[0m                                 max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m      6\u001b[0m                                 temperature\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[0;32m      7\u001b[0m                                 top_p\u001b[39m=\u001b[39;49m\u001b[39m0.97\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m                                 top_k\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m,\n\u001b[0;32m      9\u001b[0m                                 return_dict_in_generate\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     10\u001b[0m                                 output_attentions\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     11\u001b[0m                                 output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     12\u001b[0m                                 output_scores\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\alpaca_lora_4bit\\amp_wrapper.py:18\u001b[0m, in \u001b[0;36mAMPWrapper.autocast_generate\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mautocast_generate\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     17\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions):\n\u001b[1;32m---> 18\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnon_autocast_generate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\transformers\\generation\\utils.py:1574\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[0;32m   1566\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1567\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1568\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1569\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1570\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1571\u001b[0m     )\n\u001b[0;32m   1573\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[1;32m-> 1574\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[0;32m   1575\u001b[0m         input_ids,\n\u001b[0;32m   1576\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1577\u001b[0m         logits_warper\u001b[39m=\u001b[39mlogits_warper,\n\u001b[0;32m   1578\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1579\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1580\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1581\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1582\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1583\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1584\u001b[0m         streamer\u001b[39m=\u001b[39mstreamer,\n\u001b[0;32m   1585\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1586\u001b[0m     )\n\u001b[0;32m   1588\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[0;32m   1589\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m generation_config\u001b[39m.\u001b[39mnum_beams:\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\transformers\\generation\\utils.py:2621\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2618\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2620\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2621\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   2622\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2623\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   2624\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   2625\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2626\u001b[0m )\n\u001b[0;32m   2628\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2629\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\accelerate-0.20.3-py3.10.egg\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:691\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    688\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m    690\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 691\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[0;32m    692\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m    693\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    694\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    695\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    696\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    697\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    698\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    699\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    700\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    701\u001b[0m )\n\u001b[0;32m    703\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    704\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\accelerate-0.20.3-py3.10.egg\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:579\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    571\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    572\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[0;32m    573\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    577\u001b[0m     )\n\u001b[0;32m    578\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 579\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[0;32m    580\u001b[0m         hidden_states,\n\u001b[0;32m    581\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    582\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    583\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m    584\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    585\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    586\u001b[0m     )\n\u001b[0;32m    588\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    590\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\accelerate-0.20.3-py3.10.egg\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:306\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    304\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m    305\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 306\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[0;32m    307\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[0;32m    309\u001b[0m outputs \u001b[39m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m, in \u001b[0;36mbuild_forward_check_nan_inf.<locals>._func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39misnan(tensor)\u001b[39m.\u001b[39many()\u001b[39m.\u001b[39mitem():\n\u001b[0;32m      9\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot NaN in \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m input\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m result \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_nan_check_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     11\u001b[0m tensors \u001b[39m=\u001b[39m []\n\u001b[0;32m     12\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, torch\u001b[39m.\u001b[39mTensor):\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\accelerate-0.20.3-py3.10.egg\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:156\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 156\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_proj(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_fn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgate_proj(x)) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_proj(x))\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m, in \u001b[0;36mbuild_forward_check_nan_inf.<locals>._func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39misnan(tensor)\u001b[39m.\u001b[39many()\u001b[39m.\u001b[39mitem():\n\u001b[0;32m      9\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot NaN in \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m input\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m result \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_nan_check_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     11\u001b[0m tensors \u001b[39m=\u001b[39m []\n\u001b[0;32m     12\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, torch\u001b[39m.\u001b[39mTensor):\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\accelerate-0.20.3-py3.10.egg\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m, in \u001b[0;36m_patched_linear_forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable_bias:\n\u001b[0;32m     24\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39misnan(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\u001b[39m.\u001b[39many()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m---> 25\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39misinf(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\u001b[39m.\u001b[39many()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     26\u001b[0m     out \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\n\u001b[0;32m     27\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39misnan(out)\u001b[39m.\u001b[39many()\u001b[39m.\u001b[39mitem()\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     generated = model.generate(inputs=batch[\"input_ids\"],\n",
    "                                do_sample=True, use_cache=True,\n",
    "                                repetition_penalty=1.1,\n",
    "                                max_new_tokens=1,\n",
    "                                temperature=0.5,\n",
    "                                top_p=0.97,\n",
    "                                top_k=40,\n",
    "                                return_dict_in_generate=True,\n",
    "                                output_attentions=False,\n",
    "                                output_hidden_states=False,\n",
    "                                output_scores=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
