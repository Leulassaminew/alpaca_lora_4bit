{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\tqdm-4.65.0-py3.10.egg\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton not found. Please run \"pip install triton\".\n",
      "Using CUDA implementation.\n"
     ]
    }
   ],
   "source": [
    "from alpaca_lora_4bit.autograd_4bit import load_llama_model_4bit_low_ram, Autograd4bitQuantLinear, switch_backend_to\n",
    "\n",
    "switch_backend_to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca_lora_4bit import autograd_4bit\n",
    "from alpaca_lora_4bit import matmul_utils_4bit as mm4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _patched_linear_forward(self, x):\n",
    "    print(\"LINEAR\", self)\n",
    "    assert not torch.isnan(x).any().item()\n",
    "    assert not torch.isinf(x).any().item()\n",
    "    if self.bits == 4:\n",
    "        if torch.is_grad_enabled():\n",
    "            out = autograd_4bit.AutogradMatmul4bit.apply(x, self.qweight, self.scales,\n",
    "                                        self.qzeros if not self.is_v1_model else self.zeros,\n",
    "                                        self.g_idx, self.bits, self.maxq)\n",
    "            assert not torch.isnan(out).any().item()\n",
    "            assert not torch.isinf(out).any().item()\n",
    "        else:\n",
    "            out = autograd_4bit.matmul4bit_with_backend(x, self.qweight, self.scales,\n",
    "                                        self.qzeros if not self.is_v1_model else self.zeros,\n",
    "                                        self.g_idx, self.bits, self.maxq, self.groupsize)\n",
    "            assert not torch.isnan(out).any().item()\n",
    "            assert not torch.isinf(out).any().item()\n",
    "    elif self.bits == 2:\n",
    "        raise NotImplementedError(\"Debugging 4-bit case\")\n",
    "        out = AutogradMatmul2bit.apply(x, self.qweight, self.scales, self.qzeros, self.g_idx, self.bits, self.maxq)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Debugging 4-bit case\")\n",
    "        raise ValueError('Unsupported bitwidth.')\n",
    "    if not self.disable_bias:\n",
    "        assert not torch.isnan(self.bias).any().item()\n",
    "        assert not torch.isinf(self.bias).any().item()\n",
    "        out += self.bias\n",
    "        assert not torch.isnan(out).any().item()\n",
    "        assert not torch.isinf(out).any().item()\n",
    "    return out\n",
    "\n",
    "\n",
    "autograd_4bit.Autograd4bitQuantLinear.forward = _patched_linear_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _patched_matmul4bit_with_backend(x, qweight, scales, qzeros, g_idx, bits, maxq, groupsize=None):\n",
    "    assert not torch.isinf(x).any().item()\n",
    "    assert not torch.isnan(x).any().item()\n",
    "    out = mm4b.matmul4bit(x, qweight, scales, qzeros, g_idx, groupsize)\n",
    "    assert not torch.isinf(out).any().item()\n",
    "    assert not torch.isnan(out).any().item()\n",
    "    return out\n",
    "\n",
    "\n",
    "autograd_4bit.matmul4bit_with_backend = _patched_matmul4bit_with_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_forward_check_nan_inf(name, module):\n",
    "    def _func(*args, **kwargs):\n",
    "        input_tensors = [item for item in args if isinstance(item, torch.Tensor)] + \\\n",
    "            [item for item in kwargs.values() if isinstance(item, torch.Tensor)]\n",
    "        for tensor in input_tensors:\n",
    "            if torch.isinf(tensor).any().item():\n",
    "                raise ValueError(f\"Got Inf in {name} input\")\n",
    "            if torch.isnan(tensor).any().item():\n",
    "                raise ValueError(f\"Got NaN in {name} input\")\n",
    "        result = module._nan_check_old_forward(*args, **kwargs)\n",
    "        tensors = []\n",
    "        if isinstance(result, torch.Tensor):\n",
    "            tensors = result\n",
    "        if isinstance(result, tuple) or isinstance(result, list):\n",
    "            tensors = [item for item in result if isinstance(item, torch.Tensor)]\n",
    "        if isinstance(result, dict):\n",
    "            tensors = [item for item in result.values() if isinstance(item, torch.Tensor)]\n",
    "        for tensor in tensors:\n",
    "            if torch.isinf(tensor).any().item():\n",
    "                raise ValueError(f\"Got Inf in {name} output\")\n",
    "            if torch.isnan(tensor).any().item():\n",
    "                raise ValueError(f\"Got NaN in {name} output\")\n",
    "        return result\n",
    "    \n",
    "    return _func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_forward_check_nan_inf(module, name):\n",
    "    module._nan_check_old_forward = module.forward\n",
    "    module.forward = build_forward_check_nan_inf(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The safetensors archive passed at vicuna-7B-GPTQ-4bit-128g/vicuna-7B-GPTQ-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model in 4.59 seconds.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_llama_model_4bit_low_ram(\"vicuna-7B-GPTQ-4bit-128g\",\n",
    "                                                 \"vicuna-7B-GPTQ-4bit-128g/vicuna-7B-GPTQ-4bit-128g.safetensors\",\n",
    "                                                 groupsize=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_forward_check_nan_inf(model.model.embed_tokens, \"embed_tokens\")\n",
    "for i in range(len(model.model.layers)):\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].self_attn.q_proj, f\"Layer {i} self-attention q_proj\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].self_attn.k_proj, f\"Layer {i} self-attention k_proj\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].self_attn.v_proj, f\"Layer {i} self-attention v_proj\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].self_attn.o_proj, f\"Layer {i} self-attention o_proj\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].self_attn.rotary_emb, f\"Layer {i} self-attention rotary_emb\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].self_attn, f\"Layer {i} self-attention itself\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].mlp.gate_proj, f\"Layer {i} mlp gate_proj\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].mlp.down_proj, f\"Layer {i} mlp down_proj\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].mlp.up_proj, f\"Layer {i} mlp up_proj\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].mlp.act_fn, f\"Layer {i} mlp act_fn\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].mlp, f\"Layer {i} mlp itself\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].input_layernorm, f\"Layer {i} input_layernorm\")\n",
    "    wrap_forward_check_nan_inf(model.model.layers[i].post_attention_layernorm, f\"Layer {i} post_attention_layernorm\")\n",
    "wrap_forward_check_nan_inf(model.model.norm, \"norm\")\n",
    "wrap_forward_check_nan_inf(model.lm_head, \"lm_head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.half()\n",
    "for n, m in model.named_modules():\n",
    "    if isinstance(m, Autograd4bitQuantLinear):\n",
    "        if m.is_v1_model:\n",
    "            m.zeros = m.zeros.half()\n",
    "        m.scales = m.scales.half()\n",
    "        m.bias = m.bias.half()\n",
    "model.tie_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca_lora_4bit.amp_wrapper import AMPWrapper\n",
    "wrapper = AMPWrapper(model)\n",
    "wrapper.apply_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''I think the meaning of life is'''\n",
    "batch = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "batch = {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m m\u001b[39m.\u001b[39mdisable_bias:\n\u001b[0;32m      5\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39misnan(m\u001b[39m.\u001b[39mbias)\u001b[39m.\u001b[39many()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m----> 6\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39misinf(m\u001b[39m.\u001b[39mbias)\u001b[39m.\u001b[39many()\u001b[39m.\u001b[39mitem()\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "modules = list(model.modules())\n",
    "modules = [m for m in modules if isinstance(m, autograd_4bit.Autograd4bitQuantLinear)]\n",
    "for i, m in enumerate(modules):\n",
    "    if not m.disable_bias:\n",
    "        assert not torch.isnan(m.bias).any().item()\n",
    "        assert not torch.isinf(m.bias).any().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "     generated = model.generate(inputs=batch[\"input_ids\"],\n",
    "                                do_sample=True, use_cache=True,\n",
    "                                repetition_penalty=1.1,\n",
    "                                max_new_tokens=1,\n",
    "                                temperature=0.5,\n",
    "                                top_p=0.97,\n",
    "                                top_k=40,\n",
    "                                return_dict_in_generate=True,\n",
    "                                output_attentions=False,\n",
    "                                output_hidden_states=False,\n",
    "                                output_scores=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "     generated = model.generate(inputs=batch[\"input_ids\"],\n",
    "                                do_sample=True, use_cache=True,\n",
    "                                repetition_penalty=1.1,\n",
    "                                max_new_tokens=1,\n",
    "                                temperature=0.5,\n",
    "                                top_p=0.97,\n",
    "                                top_k=40,\n",
    "                                return_dict_in_generate=True,\n",
    "                                output_attentions=False,\n",
    "                                output_hidden_states=False,\n",
    "                                output_scores=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
